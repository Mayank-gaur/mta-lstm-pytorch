{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTA-LSTM-PyTorch\n",
    "\n",
    "This is an implementation of the paper [Topic-to-Essay Generation with Neural Networks](http://ir.hit.edu.cn/~xcfeng/xiaocheng%20Feng's%20Homepage_files/final-topic-essay-generation.pdf). The original work can be found [here](https://github.com/hit-computer/MTA-LSTM), which is implemented in TensorFlow and is totally out-of-date, further more, the owner doesn't seem to maintain it anymore. Therefore, I decided to re-implement it in a simple yet powerful framework, PyTorch.\n",
    "\n",
    "In this notebook, I'll show you how to build a neural network proposed in the paper step by step from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "The followings are some packages that'll be used in this work. Make sure you have them installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, autograd, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import Parameter, LayerNorm\n",
    "from torch.autograd import Variable\n",
    "import torch.jit as jit\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from collections import namedtuple\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "# !wget \"https://noto-website-2.storage.googleapis.com/pkgs/NotoSansCJKtc-hinted.zip\"\n",
    "# !unzip NotoSansCJKtc-hinted.zip\n",
    "# !sudo mv NotoSansCJKtc-Black.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansCJKtc-Bold.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansCJKtc-DemiLight.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansCJKtc-Light.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansCJKtc-Medium.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansCJKtc-Regular.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansCJKtc-Thin.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansMonoCJKtc-Bold.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansMonoCJKtc-Regular.otf /usr/share/fonts/truetype/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "font_dirs = ['/fonts/', ]\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "font_list = font_manager.createFontList(font_files)\n",
    "font_manager.fontManager.ttflist.extend(font_list)\n",
    "path = 'NotoSansCJKtc-Regular.otf'\n",
    "fontprop = fm.FontProperties(fname=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "print('Available cuda:', torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    device_num = 1\n",
    "    deviceName = \"cuda:%d\" % device_num\n",
    "    torch.cuda.set_device(device_num)\n",
    "    print('Current device:', torch.cuda.current_device())\n",
    "else:\n",
    "    deviceName = \"cpu\"\n",
    "    \n",
    "device = torch.device(deviceName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a dictionary and pretrained embedding system\n",
    "\n",
    "Here I'm gonna load the pretrained word2vec vocab and vectors. Please refer to [this notebook]() to he how to train it.\n",
    "\n",
    "The code ```fvec.vectors``` is where we get the pretrained vectors.\n",
    "```<PAD>```, ```<BOS>```, ```<EOS>``` and ```<UNK>``` are 4 common tokens which stands for **PADding**, **Begin-Of-Sentence**, **End-Of-Sentence** and **UNKnown** respectively. We simply add them into the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/'\n",
    "fvec = KeyedVectors.load_word2vec_format(file_path+'composition_mincount_1_305000_vec_original.txt', binary=False)\n",
    "word_vec = fvec.vectors\n",
    "vocab = ['<PAD>', '<BOS>', '<EOS>', '<UNK>']\n",
    "vocab.extend(list(fvec.vocab.keys()))\n",
    "word_vec = np.concatenate((np.array([[0]*word_vec.shape[1]] * 4), word_vec))\n",
    "word_vec = torch.tensor(word_vec).float()\n",
    "del fvec\n",
    "print(\"total %d words\" % len(word_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = 'model_result_multi_layer'\n",
    "vocab_check_point = '%s/vocab.pkl' % save_folder\n",
    "word_vec_check_point = '%s/word_vec.pkl' % save_folder\n",
    "torch.save(vocab, vocab_check_point)\n",
    "torch.save(word_vec, word_vec_check_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a word-index convertor\n",
    "\n",
    "We don't want to use type of string directly when training, instead we map them to a unique index in integer. In text generation phase, we'll then convert them back to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "idx_to_word = {i: ch for i, ch in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data\n",
    "\n",
    "You can prepare for your own data, or simply use what I offered in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = []\n",
    "topics = []\n",
    "num_lines = sum(1 for line in open(file_path+'composition_zh_tw.txt', 'r'))\n",
    "with open(file_path+'composition_zh_tw.txt') as f:\n",
    "    for line in tqdm(f, total=num_lines):\n",
    "        essay, topic = line.replace('\\n', '').split(' </d> ')\n",
    "        essays.append(essay.split(' '))\n",
    "        topics.append(topic.split(' '))\n",
    "    f.close()\n",
    "    \n",
    "assert len(topics) == len(essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then map all the training and testing corpus to integer index word-by-word, with the help of our convertor. Note that we map it to ```<UNK>``` if the words in corpus are not in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus_indice = list(map(lambda x: [word_to_idx[w] if (w in word_to_idx) else word_to_idx['<UNK>'] for w in x], tqdm(essays[:300000])))\n",
    "topics_indice = list(map(lambda x: [word_to_idx[w] if (w in word_to_idx) else word_to_idx['<UNK>'] for w in x], tqdm(topics[:300000])))\n",
    "corpus_test = list(map(lambda x: [word_to_idx[w] if (w in word_to_idx) else word_to_idx['<UNK>'] for w in x], tqdm(essays[300000:305000])))\n",
    "topics_test = list(map(lambda x: [word_to_idx[w] if (w in word_to_idx) else word_to_idx['<UNK>'] for w in x], tqdm(topics[300000:305000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viewData(topics, X):\n",
    "    topics = [idx_to_word[x] for x in topics]\n",
    "    X = [idx_to_word[x] for x in X]\n",
    "    print(topics, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def shuffleData(topics_indice, corpus_indice):\n",
    "    ind_list = [i for i in range(len(topics_indice))]\n",
    "    shuffle(ind_list)\n",
    "    topics_indice = np.array(topics_indice)\n",
    "    corpus_indice = np.array(corpus_indice)\n",
    "    topics_indice = topics_indice[ind_list,]\n",
    "    corpus_indice = corpus_indice[ind_list,]\n",
    "    topics_indice = topics_indice.tolist()\n",
    "    corpus_indice = corpus_indice.tolist()\n",
    "    return topics_indice, corpus_indice\n",
    "\n",
    "# topics_indice, corpus_indice = shuffleData(topics_indice, corpus_indice)\n",
    "# viewData(topics_indice[0], corpus_indice[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in topics_indice:\n",
    "    if len(t) != 5:\n",
    "        print('less than 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know the max length of training corpus too, in order to pad sequences that aren't long enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = list(map(lambda x: len(x), corpus_indice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete useless dependencies to free up some space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del essays\n",
    "del topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch data iterator\n",
    "\n",
    "We want to iter through training data in batches and feed them into the network, and this is how we prepare for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iterator(corpus_indice, topics_indice, batch_size, num_steps):\n",
    "    epoch_size = len(corpus_indice) // batch_size\n",
    "    for i in range(epoch_size):\n",
    "        raw_data = corpus_indice[i*batch_size: (i+1)*batch_size]\n",
    "        key_words = topics_indice[i*batch_size: (i+1)*batch_size]\n",
    "        data = np.zeros((len(raw_data), num_steps+1), dtype=np.int64)\n",
    "        for i in range(batch_size):\n",
    "            doc = raw_data[i]\n",
    "            tmp = [1]\n",
    "            tmp.extend(doc)\n",
    "            tmp.extend([2])\n",
    "            tmp = np.array(tmp, dtype=np.int64)\n",
    "            _size = tmp.shape[0]\n",
    "            data[i][:_size] = tmp\n",
    "        key_words = np.array(key_words, dtype=np.int64)\n",
    "        x = data[:, 0:num_steps]\n",
    "        y = data[:, 1:]\n",
    "        mask = np.float32(x != 0)\n",
    "        x = torch.tensor(x)\n",
    "        y = torch.tensor(y)\n",
    "        mask = torch.tensor(mask)\n",
    "        key_words = torch.tensor(key_words)\n",
    "        yield(x, y, mask, key_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model: MTA-LSTM\n",
    "\n",
    "This is the most important part in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, embed_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.Ua = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.va = nn.Linear(hidden_size, 1, bias=True)\n",
    "        # to store attention scores\n",
    "        self.alphas = None\n",
    "        \n",
    "    def forward(self, query, topics, coverage_vector):\n",
    "        scores = []\n",
    "        C_t = coverage_vector.clone()\n",
    "        for i in range(topics.shape[1]):\n",
    "            proj_key = self.Ua(topics[:, i, :])\n",
    "            query = self.Wa(query)\n",
    "            scores += [self.va(torch.tanh(query + proj_key)) * C_t[:, i:i+1]]\n",
    "            \n",
    "        # stack scores\n",
    "        scores = torch.stack(scores, dim=1)\n",
    "        scores = scores.squeeze(2)\n",
    "#         print(scores.shape)\n",
    "        # turn scores to probabilities\n",
    "        alphas = F.softmax(scores, dim=1)\n",
    "        self.alphas = alphas\n",
    "        \n",
    "        # mt vector is the weighted sum of the topics\n",
    "        mt = torch.bmm(alphas.unsqueeze(1), topics)\n",
    "        mt = mt.squeeze(1)\n",
    "        \n",
    "        # mt shape: [batch x embed], alphas shape: [batch x num_keywords]\n",
    "        return mt, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size, num_layers, dropout=0.5):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # topic attention\n",
    "        self.attention = Attention(hidden_size, embed_size)\n",
    "        \n",
    "        # lstm\n",
    "        self.rnn = nn.LSTM(input_size=embed_size * 2, \n",
    "                           hidden_size=hidden_size, \n",
    "                           num_layers=num_layers, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "    def forward(self, input, output, hidden, phi, topics, coverage_vector):\n",
    "        # 1. calculate attention weight and mt\n",
    "        mt, score = self.attention(output.squeeze(0), topics, coverage_vector)\n",
    "        mt = mt.unsqueeze(1).permute(1, 0, 2)\n",
    "        \n",
    "        # 2. update coverge vector [batch x num_keywords]\n",
    "        coverage_vector = coverage_vector - score / phi\n",
    "        \n",
    "        # 3. concat input and Tt, and feed into rnn \n",
    "        output, hidden = self.rnn(torch.cat([input, mt], dim=2), hidden)\n",
    "        \n",
    "        return output, hidden, score, coverage_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTA-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMState = namedtuple('LSTMState', ['hx', 'cx'])\n",
    "class MTALSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, embed_dim, num_keywords, num_layers, weight,\n",
    "                 num_labels, bidirectional, dropout=0.5, **kwargs):\n",
    "        super(MTALSTM, self).__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_labels = num_labels\n",
    "        self.bidirectional = bidirectional\n",
    "        if num_layers <= 1:\n",
    "            self.dropout = 0\n",
    "        else:\n",
    "            self.dropout = dropout\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.Uf = nn.Linear(embed_dim * num_keywords, num_keywords, bias=False)\n",
    "        \n",
    "        # attention decoder\n",
    "        self.decoder = AttentionDecoder(hidden_size=hidden_dim, \n",
    "                                        embed_size=embed_dim, \n",
    "                                        num_layers=num_layers, \n",
    "                                        dropout=dropout)\n",
    "        \n",
    "        # adaptive softmax\n",
    "        self.adaptiveSoftmax = nn.AdaptiveLogSoftmaxWithLoss(hidden_dim, \n",
    "                                                             num_labels, \n",
    "                                                             cutoffs=[round(num_labels / 20), 4*round(num_labels / 20)])\n",
    "    \n",
    "    def forward(self, inputs, topics, output, hidden=None, mask=None, target=None, coverage_vector=None, seq_length=None):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        topics_embed = self.embedding(topics)\n",
    "        ''' calculate phi [batch x num_keywords] '''\n",
    "        phi = None\n",
    "        phi = torch.sum(mask, dim=1, keepdim=True) * torch.sigmoid(self.Uf(topics_embed.reshape(topics_embed.shape[0], -1).float()))\n",
    "        \n",
    "        # loop through sequence\n",
    "        inputs = embeddings.permute([1, 0, 2]).unbind(0)\n",
    "        output_states = []\n",
    "        attn_weight = []\n",
    "        for i in range(len(inputs)):\n",
    "            output, hidden, score, coverage_vector = self.decoder(input=inputs[i].unsqueeze(0), \n",
    "                                                                        output=output, \n",
    "                                                                        hidden=hidden, \n",
    "                                                                        phi=phi, \n",
    "                                                                        topics=topics_embed, \n",
    "                                                                        coverage_vector=coverage_vector) # [seq_len x batch x embed_size]\n",
    "            output_states += [output]\n",
    "            attn_weight += [score]\n",
    "            \n",
    "        output_states = torch.stack(output_states)\n",
    "        attn_weight = torch.stack(attn_weight)\n",
    "        \n",
    "        # calculate loss py adaptiveSoftmax\n",
    "        outputs = self.adaptiveSoftmax(output_states.reshape(-1, output_states.shape[-1]), target.t().reshape((-1,)))\n",
    "        \n",
    "        return outputs, output_states, hidden, attn_weight, coverage_vector\n",
    "    \n",
    "    def inference(self, inputs, topics, output, hidden=None, mask=None, coverage_vector=None, seq_length=None):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        topics_embed = self.embedding(topics)\n",
    "       \n",
    "        phi = None\n",
    "        phi = seq_length.float() * torch.sigmoid(self.Uf(topics_embed.reshape(topics_embed.shape[0], -1).float()))\n",
    "        \n",
    "        queries = embeddings.permute([1, 0, 2])[-1].unsqueeze(0)\n",
    "        \n",
    "        inputs = queries.permute([1, 0, 2]).unbind(0)\n",
    "        output_states = []\n",
    "        attn_weight = []\n",
    "        for i in range(len(inputs)):\n",
    "            output, hidden, score, coverage_vector = self.decoder(input=inputs[i].unsqueeze(0), \n",
    "                                                                        output=output, \n",
    "                                                                        hidden=hidden, \n",
    "                                                                        phi=phi, \n",
    "                                                                        topics=topics_embed, \n",
    "                                                                        coverage_vector=coverage_vector) # [seq_len x batch x embed_size]\n",
    "            output_states += [output]\n",
    "            attn_weight += [score]\n",
    "            \n",
    "        output_states = torch.stack(output_states)\n",
    "        attn_weight = torch.stack(attn_weight)\n",
    "        \n",
    "        outputs = self.adaptiveSoftmax.log_prob(output_states.reshape(-1, output_states.shape[-1]))\n",
    "        return outputs, output_states, hidden, attn_weight, coverage_vector\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "#         hidden = torch.zeros(num_layers, batch_size, hidden_dim)\n",
    "#         hidden = LSTMState(torch.zeros(batch_size, hidden_dim).to(device), torch.zeros(batch_size, hidden_dim).to(device))\n",
    "        hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device), \n",
    "                  torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
    "        return hidden\n",
    "    \n",
    "    def init_coverage_vector(self, batch_size, num_keywords):\n",
    "#         self.coverage_vector = torch.ones([batch_size, num_keywords]).to(device)\n",
    "        return torch.ones([batch_size, num_keywords]).to(device)\n",
    "#         print(self.coverage_vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy decode strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_topic(topics):\n",
    "    topics = [word_to_idx[x] for x in topics]\n",
    "    topics = torch.tensor(topics)\n",
    "    print(topics)\n",
    "    max_num = 5\n",
    "    size = 1\n",
    "    ans = np.zeros((size, max_num), dtype=int)\n",
    "    for i in range(size):\n",
    "        true_len = min(len(topics), max_num)\n",
    "        for j in range(true_len):\n",
    "            print(topics[i])\n",
    "            ans[i][j] = topics[i][j]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(topics, num_chars, model, idx_to_word, word_to_idx):\n",
    "    output_idx = [1]\n",
    "    topics = [word_to_idx[x] for x in topics]\n",
    "    topics = torch.tensor(topics)\n",
    "    topics = topics.reshape((1, topics.shape[0]))\n",
    "#     hidden = torch.zeros(num_layers, 1, hidden_dim)\n",
    "#     hidden = (torch.zeros(num_layers, 1, hidden_dim).to(device), torch.zeros(num_layers, 1, hidden_dim).to(device))\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    if use_gpu:\n",
    "#         hidden = hidden.cuda()\n",
    "        adaptive_softmax.to(device)\n",
    "        topics = topics.to(device)\n",
    "    coverage_vector = model.init_coverage_vector(topics.shape[0], topics.shape[1])\n",
    "    attentions = torch.zeros(num_chars, topics.shape[1])\n",
    "    for t in range(num_chars):\n",
    "        X = torch.tensor(output_idx[-1]).reshape((1, 1))\n",
    "#         X = torch.tensor(output).reshape((1, len(output)))\n",
    "        if use_gpu:\n",
    "            X = X.to(device)\n",
    "        if t == 0:\n",
    "            output = torch.zeros(1, hidden_dim).to(device)\n",
    "        else:\n",
    "            output = output.squeeze(0)\n",
    "        pred, output, hidden, attn_weight, coverage_vector = model.inference(inputs=X, topics=topics, output=output, hidden=hidden, coverage_vector=coverage_vector, seq_length=torch.tensor(50).reshape(1, 1).to(device))\n",
    "#         print(coverage_vector)\n",
    "        pred = pred.argmax(dim=1) # greedy strategy\n",
    "        attentions[t] = attn_weight[0].data\n",
    "#         pred = adaptive_softmax.predict(pred)\n",
    "        if pred[-1] == 2:\n",
    "#         if pred.argmax(dim=1)[-1] == 2:\n",
    "            break\n",
    "        else:\n",
    "            output_idx.append(int(pred[-1]))\n",
    "#             output.append(int(pred.argmax(dim=1)[-1]))\n",
    "    return(''.join([idx_to_word[i] for i in output_idx[1:]]), [idx_to_word[i] for i in output_idx[1:]], attentions[:t+1].t(), output_idx[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1, 15, 23]\n",
    "test = np.array(test, dtype=np.int64)\n",
    "mm = np.float32(test != 0)\n",
    "mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(topics, num_chars, model, idx_to_word, word_to_idx, is_sample=False):\n",
    "    output_idx = [1]\n",
    "    topics = [word_to_idx[x] for x in topics]\n",
    "    topics = torch.tensor(topics)\n",
    "    topics = topics.reshape((1, topics.shape[0]))\n",
    "#     hidden = torch.zeros(num_layers, 1, hidden_dim)\n",
    "#     hidden = (torch.zeros(num_layers, 1, hidden_dim).to(device), torch.zeros(num_layers, 1, hidden_dim).to(device))\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    if use_gpu:\n",
    "#         hidden = hidden.cuda()\n",
    "        adaptive_softmax.to(device)\n",
    "        topics = topics.to(device)\n",
    "        seq_length = torch.tensor(50).reshape(1, 1).to(device)\n",
    "    \"\"\"1\"\"\"    \n",
    "    coverage_vector = model.init_coverage_vector(topics.shape[0], topics.shape[1])\n",
    "    attentions = torch.zeros(num_chars, topics.shape[1])\n",
    "    X = torch.tensor(output_idx[-1]).reshape((1, 1)).to(device)\n",
    "    output = torch.zeros(1, hidden_dim).to(device)\n",
    "    log_prob, output, hidden, attn_weight, coverage_vector = model.inference(inputs=X, \n",
    "                                                                   topics=topics, \n",
    "                                                                   output=output, \n",
    "                                                                   hidden=hidden, \n",
    "                                                                   coverage_vector=coverage_vector, \n",
    "                                                                   seq_length=seq_length)\n",
    "    log_prob = log_prob.cpu().detach().reshape(-1).numpy()\n",
    "#     print(log_prob[10])\n",
    "    \"\"\"2\"\"\"\n",
    "    if is_sample:\n",
    "        top_indices = np.random.choice(vocab_size, beam_size, replace=False, p=np.exp(log_prob))\n",
    "    else:\n",
    "        top_indices = np.argsort(-log_prob)\n",
    "    \"\"\"3\"\"\"\n",
    "    beams = [(0.0, [idx_to_word[1]], idx_to_word[1], torch.zeros(1, topics.shape[1]), torch.ones(1, topics.shape[1]))]\n",
    "    b = beams[0]\n",
    "    beam_candidates = []\n",
    "#     print(attn_weight[0].cpu().data, coverage_vector)\n",
    "#     assert False\n",
    "    for i in range(beam_size):\n",
    "        word_idx = top_indices[i]\n",
    "        beam_candidates.append((b[0]+log_prob[word_idx], b[1]+[idx_to_word[word_idx]], word_idx, torch.cat((b[3], attn_weight[0].cpu().data), 0), torch.cat((b[4], coverage_vector.cpu().data), 0), hidden, output.squeeze(0), coverage_vector))\n",
    "    \"\"\"4\"\"\"\n",
    "    beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n",
    "    beams = beam_candidates[:beam_size] # truncate to get new beams\n",
    "    \n",
    "    for xy in range(num_chars-1):\n",
    "        beam_candidates = []\n",
    "        for b in beams:\n",
    "            \"\"\"5\"\"\"\n",
    "            X = torch.tensor(b[2]).reshape((1, 1)).to(device)\n",
    "            \"\"\"6\"\"\"\n",
    "            log_prob, output, hidden, attn_weight, coverage_vector = model.inference(inputs=X, \n",
    "                                                                           topics=topics, \n",
    "                                                                           output=b[6], \n",
    "                                                                           hidden=b[5], \n",
    "                                                                           coverage_vector=b[7], \n",
    "                                                                           seq_length=seq_length)\n",
    "            log_prob = log_prob.cpu().detach().reshape(-1).numpy()\n",
    "            \"\"\"8\"\"\"\n",
    "            if is_sample:\n",
    "                top_indices = np.random.choice(vocab_size, beam_size, replace=False, p=np.exp(log_prob))\n",
    "            else:\n",
    "                top_indices = np.argsort(-log_prob)\n",
    "            \"\"\"9\"\"\"\n",
    "            for i in range(beam_size):\n",
    "                word_idx = top_indices[i]\n",
    "                beam_candidates.append((b[0]+log_prob[word_idx], b[1]+[idx_to_word[word_idx]], word_idx, torch.cat((b[3], attn_weight[0].cpu().data), 0), torch.cat((b[4], coverage_vector.cpu().data), 0), hidden, output.squeeze(0), coverage_vector))\n",
    "        \"\"\"10\"\"\"\n",
    "        beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n",
    "        beams = beam_candidates[:beam_size] # truncate to get new beams\n",
    "    \n",
    "    \"\"\"11\"\"\"\n",
    "    if '<EOS>' in beams[0][1]:\n",
    "        first_eos = beams[0][1].index('<EOS>')\n",
    "    else:\n",
    "        first_eos = num_chars-1\n",
    "    return(''.join(beams[0][1][:first_eos]), beams[0][1][:first_eos], beams[0][3][:first_eos].t(), beams[0][4][:first_eos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.switch_backend('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "    \n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.subplots(1)\n",
    "#     cmap = 'bone'\n",
    "    cmap = 'viridis'\n",
    "    cax = ax.matshow(attentions.numpy(), cmap=cmap)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_yticklabels([''] + input_sentence.split(' '), fontproperties=fontprop, fontsize=10)\n",
    "    ax.set_xticklabels([''] + output_words, fontproperties=fontprop, fontsize=10, rotation=45)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    word_size = 0.5\n",
    "    fig.set_figheight(word_size * len(input_sentence.split(' ')))\n",
    "    fig.set_figwidth(word_size * len(output_words))\n",
    "    plt.show()\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence, method='beam_search', is_sample=False):\n",
    "    if method == 'beam_search':\n",
    "        _, output_words, attentions, coverage_vector = beam_search(input_sentence, 100, model, idx_to_word, word_to_idx, is_sample=is_sample)\n",
    "    else:\n",
    "        _, output_words, attentions, _ = predict_rnn(input_sentence, 100, model, idx_to_word, word_to_idx)\n",
    "    print('input =', ' '.join(input_sentence))\n",
    "    print('output =', ' '.join(output_words))\n",
    "#     n_digits = 3\n",
    "#     coverage_vector = torch.round(coverage_vector * 10**n_digits) / (10**n_digits)\n",
    "#     coverage_vector=np.round(coverage_vector, n_digits)\n",
    "#     print(coverage_vector.numpy())\n",
    "    showAttention(' '.join(input_sentence), output_words, attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "\n",
    "def evaluate_bleu(model, topics_test, corpus_test, num_test, method='beam_search', is_sample=False):\n",
    "    bleu_2_score = 0\n",
    "    for i in tqdm(range(len(corpus_test[:num_test]))):\n",
    "        if method == 'beam_search':\n",
    "            _, output_words, _, _ = beam_search([idx_to_word[x] for x in topics_test[i]], 100, model, idx_to_word, word_to_idx, False)\n",
    "        else:\n",
    "            _, output_words, _, _ = predict_rnn([idx_to_word[x] for x in topics_test[i]], 100, model, idx_to_word, word_to_idx)\n",
    "        bleu_2_score += sentence_bleu([[idx_to_word[x] for x in corpus_test[i] if x not in [0, 2]]], output_words, weights=(0, 1, 0, 0))\n",
    "        \n",
    "    bleu_2_score = bleu_2_score / num_test * 100\n",
    "    return bleu_2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 512\n",
    "lr = 1e-3 * 0.5\n",
    "momentum = 0.01\n",
    "num_epoch = 100\n",
    "clip_value = 0.1\n",
    "use_gpu = True\n",
    "num_layers = 2\n",
    "bidirectional = False\n",
    "batch_size = 32\n",
    "num_keywords = 5\n",
    "verbose = 1\n",
    "check_point = 5\n",
    "beam_size = 2\n",
    "is_sample = True\n",
    "vocab_size = len(vocab)\n",
    "# device = torch.device(deviceName)\n",
    "loss_function = nn.NLLLoss()\n",
    "adaptive_softmax = nn.AdaptiveLogSoftmaxWithLoss(\n",
    "    1000, len(vocab), cutoffs=[round(vocab_size / 20), 4*round(vocab_size / 20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     \"embedding_dim\": 100, \n",
    "#     \"hidden_dim\": 512, \n",
    "#     \"batch_size\": 64, \n",
    "#     \"num_keywords\": 5, \n",
    "#     \"lr\": 1e-3 * 0.5, \n",
    "#     \"momentum\": 0.01, \n",
    "#     \"num_epoch\": 100, \n",
    "#     \"clip_value\": 1, \n",
    "#     \"use_gpu\": True, \n",
    "#     \"num_layers\": 1, \n",
    "#     \"bidirectional\": False, \n",
    "#     \"verbose\": 1, \n",
    "#     \"check_point\": 5, \n",
    "#     \"beam_size\": 2, \n",
    "#     \"is_sample\": True, \n",
    "#     \"vocab_size\": len(vocab), \n",
    "#     \"device\": torch.device(deviceName)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = MTALSTM(hidden_dim=hidden_dim, embed_dim=embedding_dim, num_keywords=num_keywords, \n",
    "                num_layers=num_layers, num_labels=len(vocab), weight=word_vec, bidirectional=bidirectional)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "from radam import RAdam\n",
    "optimizer = RAdam(model.parameters(), lr=lr)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=2, min_lr=1e-7, verbose=True)\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "if use_gpu:\n",
    "#     model = nn.DataParallel(model)\n",
    "#     model = model.to(device)\n",
    "    model = model.to('cuda:1')\n",
    "    print(\"Dump to cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def params_init_uniform(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        y = 0.04\n",
    "        nn.init.uniform_(m.weight, -y, y)\n",
    "        \n",
    "model.apply(params_init_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load previous checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_num = 0\n",
    "# Type = 'best'\n",
    "Type = 'trainable'\n",
    "model_check_point = '%s/model_%s_%d.pk' % (save_folder, Type, version_num)\n",
    "optim_check_point = '%s/optim_%s_%d.pkl' % (save_folder, Type, version_num)\n",
    "loss_check_point = '%s/loss_%s_%d.pkl' % (save_folder, Type, version_num)\n",
    "epoch_check_point = '%s/epoch_%s_%d.pkl' % (save_folder, Type, version_num)\n",
    "bleu_check_point = '%s/bleu_%s_%d.pkl' % (save_folder, Type, version_num)\n",
    "loss_values = []\n",
    "epoch_values = []\n",
    "bleu_values = []\n",
    "if os.path.isfile(model_check_point):\n",
    "    print('Loading previous status (ver.%d)...' % version_num)\n",
    "    model.load_state_dict(torch.load(model_check_point, map_location='cpu'))\n",
    "    model = model.to(device)\n",
    "    optimizer.load_state_dict(torch.load(optim_check_point))\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=2, min_lr=1e-7, verbose=True)\n",
    "    loss_values = torch.load(loss_check_point)\n",
    "    epoch_values = torch.load(epoch_check_point)\n",
    "    bleu_values = torch.load(bleu_check_point)\n",
    "    print('Load successfully')\n",
    "else:\n",
    "    print(\"ver.%d doesn't exist\" % version_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluateAndShowAttention(['現在', '未來', '夢想', '科學', '文化'], method='beam_search', is_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def isnan(x):\n",
    "    return x != x\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "#     if p.grad is None:\n",
    "#         continue\n",
    "    if p.requires_grad:\n",
    "        print(name, p)\n",
    "#         p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_lr(optimizer, epoch, factor=0.1, lr_decay_epoch=60):\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * factor\n",
    "        print('lr decayed to %.4f' % optimizer.param_group[0]['lr'])\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "autograd.set_detect_anomaly(False)\n",
    "prev_epoch = 0 if not epoch_values else epoch_values[-1]\n",
    "best_bleu = 0 if not bleu_values else max(bleu_values)\n",
    "\n",
    "for epoch in range(num_epoch - prev_epoch):\n",
    "    epoch += prev_epoch\n",
    "    start = time.time()\n",
    "    num, total_loss = 0, 0\n",
    "#     optimizer = decay_lr(optimizer=optimizer, epoch=epoch+1)\n",
    "    topics_indice, corpus_indice = shuffleData(topics_indice, corpus_indice) # shuffle data at every epoch\n",
    "    data = data_iterator(corpus_indice, topics_indice, batch_size, max(length) + 1)\n",
    "    hidden = model.init_hidden(batch_size=batch_size)\n",
    "    weight = torch.ones(len(vocab))\n",
    "    weight[0] = 0\n",
    "    num_iter = len(corpus_indice) // batch_size\n",
    "    for X, Y, mask, topics in tqdm(data, total=num_iter):\n",
    "        num += 1\n",
    "#         hidden.detach_()\n",
    "        if use_gpu:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            mask = mask.to(device)\n",
    "            topics = topics.to(device)\n",
    "#             hidden = hidden.to(device)\n",
    "#             hidden[0].to(device)\n",
    "#             hidden[1].to(device)\n",
    "            loss_function = loss_function.to(device)\n",
    "            weight = weight.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # init hidden layer\n",
    "#         hidden = model.init_hidden(num_layers, batch_size, hidden_dim)\n",
    "        coverage_vector = model.init_coverage_vector(batch_size, num_keywords)\n",
    "        init_output = torch.zeros(batch_size, hidden_dim).to(device)\n",
    "        # inputs, topics, output, hidden=None, mask=None, target=None, coverage_vector=None, seq_length=None):\n",
    "        output, _, hidden, _, _ = model(inputs=X, topics=topics, output=init_output, hidden=hidden, mask=mask, target=Y, coverage_vector=coverage_vector)\n",
    "#         output, hidden = model(X, topics)\n",
    "        hidden[0].detach_()\n",
    "        hidden[1].detach_()\n",
    "        \n",
    "        loss = (-output.output).reshape((-1, batch_size)).t() * mask\n",
    "#         loss = loss.sum(dim=1)\n",
    "        loss = loss.sum(dim=1) / mask.sum(dim=1)\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        norm = 0.0\n",
    "#         norm = nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        nn.utils.clip_grad_value_(model.parameters(), 1)\n",
    "            \n",
    "        optimizer.step()\n",
    "        total_loss += float(loss.item())\n",
    "        \n",
    "        if np.isnan(total_loss):\n",
    "            for name, p in model.named_parameters():\n",
    "                if p.grad is None:\n",
    "                    continue \n",
    "                print(name, p)\n",
    "            assert False, \"Gradient explode\"\n",
    "    \n",
    "    one_iter_loss = np.mean(total_loss)\n",
    "    lr_scheduler.step(one_iter_loss)\n",
    "#     print(\"One iteration loss {:.3f}\".format(one_iter_loss))\n",
    "    \n",
    "    # validation\n",
    "    bleu_score = 0\n",
    "    num_test = 500\n",
    "    bleu_score = evaluate_bleu(model, topics_test, corpus_test, num_test=num_test, method='predict_rnn', is_sample=False)\n",
    "    \n",
    "    bleu_values.append(bleu_score)\n",
    "    loss_values.append(total_loss / num)\n",
    "    epoch_values.append(epoch+1)\n",
    "    \n",
    "    # save checkpoint\n",
    "    if ((epoch + 1) % check_point == 0) or (epoch == (num_epoch - 1)) or epoch+1 > 90 or bleu_score > 4:\n",
    "        model_check_point = '%s/model_trainable_%d.pk' % (save_folder, epoch+1)\n",
    "        optim_check_point = '%s/optim_trainable_%d.pkl' % (save_folder, epoch+1)\n",
    "        loss_check_point = '%s/loss_trainable_%d.pkl' % (save_folder, epoch+1)\n",
    "        epoch_check_point = '%s/epoch_trainable_%d.pkl' % (save_folder, epoch+1)\n",
    "        bleu_check_point = '%s/bleu_trainable_%d.pkl' % (save_folder, epoch+1)\n",
    "        torch.save(model.state_dict(), model_check_point)\n",
    "        torch.save(optimizer.state_dict(), optim_check_point)\n",
    "        torch.save(loss_values, loss_check_point)\n",
    "        torch.save(epoch_values, epoch_check_point)\n",
    "        torch.save(bleu_values, bleu_check_point)\n",
    "    \n",
    "    # save current best result\n",
    "    if bleu_score > best_bleu:\n",
    "        best_bleu = bleu_score\n",
    "        print('current best bleu: %.4f' % best_bleu)\n",
    "        model_check_point = '%s/model_best_%d.pk' % (save_folder, epoch+1)\n",
    "        optim_check_point = '%s/optim_best_%d.pkl' % (save_folder, epoch+1)\n",
    "        loss_check_point = '%s/loss_best_%d.pkl' % (save_folder, epoch+1)\n",
    "        epoch_check_point = '%s/epoch_best_%d.pkl' % (save_folder, epoch+1)\n",
    "        bleu_check_point = '%s/bleu_best_%d.pkl' % (save_folder, epoch+1)\n",
    "        torch.save(model.state_dict(), model_check_point)\n",
    "        torch.save(optimizer.state_dict(), optim_check_point)\n",
    "        torch.save(loss_values, loss_check_point)\n",
    "        torch.save(epoch_values, epoch_check_point)\n",
    "        torch.save(bleu_values, bleu_check_point)\n",
    "        \n",
    "    # calculate time\n",
    "    end = time.time()\n",
    "    s = end - since\n",
    "    h = math.floor(s / 3600)\n",
    "    m = s - h * 3600\n",
    "    m = math.floor(m / 60)\n",
    "    s -= (m * 60 + h * 3600)\n",
    "    \n",
    "    # verbose\n",
    "    if ((epoch + 1) % verbose == 0) or (epoch == (num_epoch - 1)):\n",
    "        print('epoch %d/%d, loss %.4f, norm %.4f, predict bleu: %.4f, time %.3fs, since %dh %dm %ds'\n",
    "              % (epoch + 1, num_epoch, total_loss / num, norm, bleu_score, end - start, h, m, s))\n",
    "        \n",
    "        evaluateAndShowAttention(['現在', '未來', '夢想', '科學', '文化'], method='beam_search', is_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_values, loss_values, 'b')\n",
    "plt.title('loss vs. epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_values, bleu_values, 'r')\n",
    "plt.title('bleu vs. epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('bleu score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 5000\n",
    "bleu_score = evaluate_bleu(model, topics_test, corpus_test, num_test=num_test, method='predict_rnn', is_sample=False)\n",
    "bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateAndShowAttention(['媽媽', '希望', '長大', '孩子', '母愛'], method='beam_search', is_sample=True)\n",
    "evaluateAndShowAttention(['現在', '未來', '夢想', '科學', '文化'], method='beam_search', is_sample=True)\n",
    "evaluateAndShowAttention(['春天', '來臨', '田野', '聆聽', '小路'], method='beam_search', is_sample=True)\n",
    "evaluateAndShowAttention(['子女', '父母', '父愛', '無比', '溫暖'], method='beam_search', is_sample=True)\n",
    "evaluateAndShowAttention(['信念', '人生', '失落', '心靈', '不屈'], method='beam_search', is_sample=True)\n",
    "evaluateAndShowAttention(['體會', '母親', '滴水之恩', '母愛', '養育之恩'], method='beam_search', is_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
